evaluation/env_infos/initial/reward_ctrl Max,evaluation/env_infos/initial/reward_ctrl Min,evaluation/env_infos/reward_run Std,exploration/Actions Std,exploration/Actions Min,evaluation/env_infos/initial/reward_run Max,exploration/Actions Mean,trainer/Log Pis Std,evaluation/Returns Std,evaluation/num paths total,evaluation/Actions Std,evaluation/env_infos/initial/reward_run Mean,evaluation/Returns Min,evaluation/env_infos/final/reward_ctrl Mean,exploration/env_infos/reward_ctrl Std,evaluation/Rewards Std,exploration/env_infos/final/reward_run Std,exploration/Returns Mean,trainer/Q1 Predictions Min,exploration/env_infos/initial/reward_run Std,exploration/Average Returns,evaluation/Actions Min,evaluation/env_infos/final/reward_ctrl Min,evaluation/path length Max,trainer/QF1 Loss,trainer/Q1 Predictions Max,time/epoch (s),exploration/num paths total,evaluation/Rewards Min,exploration/env_infos/final/reward_run Mean,evaluation/env_infos/reward_run Max,evaluation/Num Paths,exploration/env_infos/final/reward_run Max,evaluation/Actions Mean,exploration/env_infos/reward_run Min,trainer/Q2 Predictions Max,trainer/Alpha,evaluation/env_infos/final/reward_run Max,exploration/env_infos/initial/reward_ctrl Min,exploration/path length Max,evaluation/Rewards Mean,exploration/env_infos/reward_ctrl Max,time/training (s),evaluation/env_infos/reward_ctrl Mean,trainer/Log Pis Max,exploration/env_infos/final/reward_ctrl Min,evaluation/env_infos/reward_ctrl Max,evaluation/env_infos/reward_run Mean,trainer/Alpha Loss,exploration/env_infos/initial/reward_run Min,trainer/Q1 Predictions Mean,trainer/Policy mu Mean,exploration/env_infos/final/reward_run Min,trainer/Q1 Predictions Std,evaluation/env_infos/initial/reward_run Min,evaluation/env_infos/final/reward_ctrl Max,trainer/Log Pis Mean,exploration/env_infos/initial/reward_run Max,trainer/Q2 Predictions Min,exploration/path length Mean,evaluation/env_infos/final/reward_ctrl Std,trainer/Policy log std Min,exploration/Rewards Mean,trainer/Policy mu Min,trainer/Policy Loss,exploration/Rewards Max,evaluation/Average Returns,evaluation/Returns Max,trainer/Policy mu Max,evaluation/env_infos/initial/reward_run Std,evaluation/path length Min,exploration/env_infos/final/reward_ctrl Mean,evaluation/env_infos/final/reward_run Std,exploration/env_infos/reward_ctrl Mean,trainer/Log Pis Min,trainer/Q Targets Mean,trainer/Q2 Predictions Mean,trainer/Policy mu Std,trainer/Q Targets Max,evaluation/env_infos/reward_run Min,exploration/env_infos/initial/reward_ctrl Mean,trainer/Q2 Predictions Std,trainer/Policy log std Max,trainer/Policy log std Mean,Epoch,exploration/num steps total,evaluation/env_infos/reward_ctrl Min,exploration/env_infos/reward_ctrl Min,evaluation/Returns Mean,trainer/Q Targets Std,trainer/QF2 Loss,exploration/Rewards Std,time/saving (s),evaluation/num steps total,exploration/env_infos/reward_run Max,exploration/env_infos/initial/reward_run Mean,exploration/Rewards Min,evaluation/env_infos/initial/reward_ctrl Mean,exploration/Returns Std,time/data storing (s),evaluation/Actions Max,evaluation/env_infos/initial/reward_ctrl Std,exploration/env_infos/initial/reward_ctrl Std,exploration/Returns Max,evaluation/env_infos/final/reward_run Mean,time/logging (s),trainer/Q Targets Min,evaluation/env_infos/reward_ctrl Std,evaluation/path length Std,exploration/env_infos/reward_run Std,exploration/env_infos/final/reward_ctrl Std,trainer/Policy log std Std,evaluation/path length Mean,evaluation/env_infos/final/reward_run Min,time/evaluation sampling (s),replay_buffer/size,exploration/path length Std,exploration/Num Paths,exploration/Actions Max,exploration/env_infos/initial/reward_ctrl Max,exploration/path length Min,exploration/env_infos/reward_run Mean,exploration/Returns Min,evaluation/Rewards Max,time/total (s),time/exploration sampling (s),exploration/env_infos/final/reward_ctrl Max
-8.004727533261758e-07,-9.180367669614498e-07,0.01919611695260614,0.62978005,-0.9988473,0.3818386615667871,-0.0038621293,0.5137837,0.9299322541714417,5,0.0012094202,0.1685033993844173,-0.8207301876287972,-8.89547936822055e-07,0.07580632773970299,0.019196118869612526,0.0,-473.9978940574437,0.008561524,0.0,-473.9978940574437,-0.003712009,-8.89547936822055e-07,1000,12.924858,0.034590676,12.772026241999924,2,-0.3345653975407637,-0.07511089109677016,0.41480425731420534,5,-0.07511089109677016,-0.0001507233,-2.174808806856937,0.021385573,0.9997000694274902,2.7755575615628914e-16,-0.27662389278411864,1000,0.0003556401206616624,-0.019384300708770754,11.308949776999953,-8.912338759182603e-07,-2.562231,-0.2478637218475342,-7.351711246883497e-07,0.0003565313545375807,-0.0,-0.21193505371972815,0.017090593,-0.0004055684,-0.07511089109677016,0.0045847907,0.04248498585310824,-8.89547936822055e-07,-3.9946156,-0.21193505371972815,-0.007902615,1000.0,0.0,-0.0061053257,-0.4739978940574436,-0.008185086,-4.0009003,1.4312656176324263,0.3556401206616438,1.6033358491743517,0.0057771043,0.13185037237232147,1000,-0.2478637218475342,1.0949389893418967e-16,-0.23798266709148885,-5.207408,3.510104,0.0063918047,0.0024747448,6.2804947,-0.33456454667827495,-0.27662389278411864,0.005761299,0.0084578,0.0012590908,0,2000,-3.0975752451922745e-06,-0.49671425819396975,0.3556401206616438,0.8508347,12.998057,0.6946757983525323,0.007851924000078725,5000,1.6962489565872074,-0.21193505371972815,-2.4414366430843537,-8.566687756683678e-07,0.0,0.004829826999980469,0.0035101273,3.851937476252809e-08,0.0,-473.9978940574437,7.979727989493313e-17,0.01521020799998496,0.19798326,4.010074933585244e-08,0.0,0.6904039910018254,0.0,0.0020348863,1000.0,0.0,1.123092882999913,2000,0.0,1,0.9983984,-0.27662389278411864,1000,-0.23601522696595476,-473.9978940574437,0.414803008575888,13.210033624000062,0.3120916230000148,-0.2478637218475342
