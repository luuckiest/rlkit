replay_buffer/size,trainer/QF Loss,trainer/Policy Loss,trainer/Raw Policy Loss,trainer/Preactivation Policy Loss,trainer/Q Predictions Mean,trainer/Q Predictions Std,trainer/Q Predictions Max,trainer/Q Predictions Min,trainer/Q Targets Mean,trainer/Q Targets Std,trainer/Q Targets Max,trainer/Q Targets Min,trainer/Bellman Errors Mean,trainer/Bellman Errors Std,trainer/Bellman Errors Max,trainer/Bellman Errors Min,trainer/Policy Action Mean,trainer/Policy Action Std,trainer/Policy Action Max,trainer/Policy Action Min,exploration/num steps total,exploration/num paths total,exploration/path length Mean,exploration/path length Std,exploration/path length Max,exploration/path length Min,exploration/Rewards Mean,exploration/Rewards Std,exploration/Rewards Max,exploration/Rewards Min,exploration/Returns Mean,exploration/Returns Std,exploration/Returns Max,exploration/Returns Min,exploration/Actions Mean,exploration/Actions Std,exploration/Actions Max,exploration/Actions Min,exploration/Num Paths,exploration/Average Returns,exploration/env_infos/final/reward_run Mean,exploration/env_infos/final/reward_run Std,exploration/env_infos/final/reward_run Max,exploration/env_infos/final/reward_run Min,exploration/env_infos/initial/reward_run Mean,exploration/env_infos/initial/reward_run Std,exploration/env_infos/initial/reward_run Max,exploration/env_infos/initial/reward_run Min,exploration/env_infos/reward_run Mean,exploration/env_infos/reward_run Std,exploration/env_infos/reward_run Max,exploration/env_infos/reward_run Min,exploration/env_infos/final/reward_ctrl Mean,exploration/env_infos/final/reward_ctrl Std,exploration/env_infos/final/reward_ctrl Max,exploration/env_infos/final/reward_ctrl Min,exploration/env_infos/initial/reward_ctrl Mean,exploration/env_infos/initial/reward_ctrl Std,exploration/env_infos/initial/reward_ctrl Max,exploration/env_infos/initial/reward_ctrl Min,exploration/env_infos/reward_ctrl Mean,exploration/env_infos/reward_ctrl Std,exploration/env_infos/reward_ctrl Max,exploration/env_infos/reward_ctrl Min,evaluation/num steps total,evaluation/num paths total,evaluation/path length Mean,evaluation/path length Std,evaluation/path length Max,evaluation/path length Min,evaluation/Rewards Mean,evaluation/Rewards Std,evaluation/Rewards Max,evaluation/Rewards Min,evaluation/Returns Mean,evaluation/Returns Std,evaluation/Returns Max,evaluation/Returns Min,evaluation/Actions Mean,evaluation/Actions Std,evaluation/Actions Max,evaluation/Actions Min,evaluation/Num Paths,evaluation/Average Returns,evaluation/env_infos/final/reward_run Mean,evaluation/env_infos/final/reward_run Std,evaluation/env_infos/final/reward_run Max,evaluation/env_infos/final/reward_run Min,evaluation/env_infos/initial/reward_run Mean,evaluation/env_infos/initial/reward_run Std,evaluation/env_infos/initial/reward_run Max,evaluation/env_infos/initial/reward_run Min,evaluation/env_infos/reward_run Mean,evaluation/env_infos/reward_run Std,evaluation/env_infos/reward_run Max,evaluation/env_infos/reward_run Min,evaluation/env_infos/final/reward_ctrl Mean,evaluation/env_infos/final/reward_ctrl Std,evaluation/env_infos/final/reward_ctrl Max,evaluation/env_infos/final/reward_ctrl Min,evaluation/env_infos/initial/reward_ctrl Mean,evaluation/env_infos/initial/reward_ctrl Std,evaluation/env_infos/initial/reward_ctrl Max,evaluation/env_infos/initial/reward_ctrl Min,evaluation/env_infos/reward_ctrl Mean,evaluation/env_infos/reward_ctrl Std,evaluation/env_infos/reward_ctrl Max,evaluation/env_infos/reward_ctrl Min,time/data storing (s),time/evaluation sampling (s),time/exploration sampling (s),time/logging (s),time/saving (s),time/training (s),time/epoch (s),time/total (s),Epoch
11000,0.18623064,0.0069150412,0.0069150412,0.0,-0.0069104275,0.0024843512,-0.0008678441,-0.014749002,-0.20107044,0.38497287,0.800413,-1.2535297,0.18623063,0.30057055,1.5538841,4.996599e-06,0.0024766515,0.0059129023,0.018068407,-0.013550897,11000,11,1000.0,0.0,1000,1000,-0.1876936185851856,0.468746061047218,1.34794578430268,-1.9020702249872266,-187.69361858518553,0.0,-187.69361858518553,-187.69361858518553,0.0031492303976783423,0.5320143337270578,1.0,-1.0,1,-187.69361858518553,0.20393549000383704,0.0,0.20393549000383704,0.20393549000383704,0.18860793630317785,0.0,0.18860793630317785,0.18860793630317785,-0.0178641172192999,0.4701434378131592,1.5419519447743202,-1.6926587361102408,-0.2908417723801377,0.0,-0.2908417723801377,-0.2908417723801377,-0.01869327808335864,0.0,-0.01869327808335864,-0.01869327808335864,-0.16982950136588568,0.07416026438128194,-0.01869327808335864,-0.4231081258032884,1000,1,1000.0,0.0,1000,1000,-0.0005166536289351553,0.010610101917438853,0.04243460184240326,-0.16679115098431538,-0.5166536289351552,0.0,-0.5166536289351552,-0.5166536289351552,0.0021685262,0.004467464,0.008298923,-0.0083529,1,-0.5166536289351552,0.0,0.0,0.0,0.0,0.03727246949342844,0.0,0.03727246949342844,0.03727246949342844,-0.0005018572818368467,0.010610053571950323,0.04244876917670559,-0.1667762148752419,-1.4796124014537782e-05,0.0,-1.4796124014537782e-05,-1.4796124014537782e-05,-1.4533613284584135e-05,0.0,-1.4533613284584135e-05,-1.4533613284584135e-05,-1.479634709830862e-05,1.8773794717139998e-07,-1.4026096323505044e-05,-2.0019635849166664e-05,0.0039028730025165714,0.19957487599822343,0.2212135900008434,0.005949595000856789,0.006531138002173975,5.899758306997683,6.336930379002297,8.7144140470009,0
