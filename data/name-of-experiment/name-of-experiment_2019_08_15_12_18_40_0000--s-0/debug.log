2019-08-15 12:18:52.556642 EDT | [name-of-experiment_2019_08_15_12_18_40_0000--s-0] Epoch 0 finished
----------------------------------------------  ---------------
replay_buffer/size                              11000
trainer/QF Loss                                     0.176466
trainer/Policy Loss                                -0.000846647
trainer/Raw Policy Loss                            -0.000846647
trainer/Preactivation Policy Loss                   0
trainer/Q Predictions Mean                          0.000929314
trainer/Q Predictions Std                           0.00188239
trainer/Q Predictions Max                           0.00571589
trainer/Q Predictions Min                          -0.0032841
trainer/Q Targets Mean                             -0.180065
trainer/Q Targets Std                               0.379306
trainer/Q Targets Max                               1.00771
trainer/Q Targets Min                              -1.45778
trainer/Bellman Errors Mean                         0.176466
trainer/Bellman Errors Std                          0.274463
trainer/Bellman Errors Max                          2.12255
trainer/Bellman Errors Min                          4.38855e-06
trainer/Policy Action Mean                          0.00237488
trainer/Policy Action Std                           0.00346944
trainer/Policy Action Max                           0.0117191
trainer/Policy Action Min                          -0.00608058
exploration/num steps total                     11000
exploration/num paths total                        11
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.158742
exploration/Rewards Std                             0.327934
exploration/Rewards Max                             0.983924
exploration/Rewards Min                            -1.85981
exploration/Returns Mean                         -158.742
exploration/Returns Std                             0
exploration/Returns Max                          -158.742
exploration/Returns Min                          -158.742
exploration/Actions Mean                           -0.0157154
exploration/Actions Std                             0.538667
exploration/Actions Max                             1
exploration/Actions Min                            -1
exploration/Num Paths                               1
exploration/Average Returns                      -158.742
exploration/env_infos/final/reward_run Mean         0.085843
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.085843
exploration/env_infos/final/reward_run Min          0.085843
exploration/env_infos/initial/reward_run Mean      -0.122015
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max       -0.122015
exploration/env_infos/initial/reward_run Min       -0.122015
exploration/env_infos/reward_run Mean               0.0155033
exploration/env_infos/reward_run Std                0.314807
exploration/env_infos/reward_run Max                1.16417
exploration/env_infos/reward_run Min               -1.68268
exploration/env_infos/final/reward_ctrl Mean       -0.162226
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.162226
exploration/env_infos/final/reward_ctrl Min        -0.162226
exploration/env_infos/initial/reward_ctrl Mean     -0.0542031
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.0542031
exploration/env_infos/initial/reward_ctrl Min      -0.0542031
exploration/env_infos/reward_ctrl Mean             -0.174245
exploration/env_infos/reward_ctrl Std               0.0751957
exploration/env_infos/reward_ctrl Max              -0.0115244
exploration/env_infos/reward_ctrl Min              -0.492442
evaluation/num steps total                       1000
evaluation/num paths total                          1
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.000396893
evaluation/Rewards Std                              0.00941627
evaluation/Rewards Max                              0.169128
evaluation/Rewards Min                             -0.0918811
evaluation/Returns Mean                             0.396893
evaluation/Returns Std                              0
evaluation/Returns Max                              0.396893
evaluation/Returns Min                              0.396893
evaluation/Actions Mean                             0.00164031
evaluation/Actions Std                              0.00244301
evaluation/Actions Max                              0.00656605
evaluation/Actions Min                             -0.0020203
evaluation/Num Paths                                1
evaluation/Average Returns                          0.396893
evaluation/env_infos/final/reward_run Mean         -1.38778e-16
evaluation/env_infos/final/reward_run Std           0
evaluation/env_infos/final/reward_run Max          -1.38778e-16
evaluation/env_infos/final/reward_run Min          -1.38778e-16
evaluation/env_infos/initial/reward_run Mean        0.0965384
evaluation/env_infos/initial/reward_run Std         0
evaluation/env_infos/initial/reward_run Max         0.0965384
evaluation/env_infos/initial/reward_run Min         0.0965384
evaluation/env_infos/reward_run Mean                0.000402088
evaluation/env_infos/reward_run Std                 0.00941632
evaluation/env_infos/reward_run Max                 0.169134
evaluation/env_infos/reward_run Min                -0.0918764
evaluation/env_infos/final/reward_ctrl Mean        -5.1971e-06
evaluation/env_infos/final/reward_ctrl Std          0
evaluation/env_infos/final/reward_ctrl Max         -5.1971e-06
evaluation/env_infos/final/reward_ctrl Min         -5.1971e-06
evaluation/env_infos/initial/reward_ctrl Mean      -4.98743e-06
evaluation/env_infos/initial/reward_ctrl Std        0
evaluation/env_infos/initial/reward_ctrl Max       -4.98743e-06
evaluation/env_infos/initial/reward_ctrl Min       -4.98743e-06
evaluation/env_infos/reward_ctrl Mean              -5.19532e-06
evaluation/env_infos/reward_ctrl Std                1.02015e-07
evaluation/env_infos/reward_ctrl Max               -3.7814e-06
evaluation/env_infos/reward_ctrl Min               -7.26528e-06
time/data storing (s)                               0.00451999
time/evaluation sampling (s)                        0.191993
time/exploration sampling (s)                       0.224979
time/logging (s)                                    0.0060065
time/saving (s)                                     0.0103603
time/training (s)                                   9.30861
time/epoch (s)                                      9.74647
time/total (s)                                     12.7734
Epoch                                               0
----------------------------------------------  ---------------
2019-08-15 12:19:02.925554 EDT | [name-of-experiment_2019_08_15_12_18_40_0000--s-0] Epoch 1 finished
----------------------------------------------  ---------------
replay_buffer/size                              12000
trainer/QF Loss                                     0.235063
trainer/Policy Loss                                -0.28093
trainer/Raw Policy Loss                            -0.28093
trainer/Preactivation Policy Loss                   0
trainer/Q Predictions Mean                          0.0157981
trainer/Q Predictions Std                           2.16766
trainer/Q Predictions Max                           6.80524
trainer/Q Predictions Min                          -7.40842
trainer/Q Targets Mean                              0.238327
trainer/Q Targets Std                               2.26076
trainer/Q Targets Max                               6.28634
trainer/Q Targets Min                              -7.52151
trainer/Bellman Errors Mean                         0.235063
trainer/Bellman Errors Std                          0.591286
trainer/Bellman Errors Max                          4.5643
trainer/Bellman Errors Min                          6.15662e-05
trainer/Policy Action Mean                         -0.103833
trainer/Policy Action Std                           0.378054
trainer/Policy Action Max                           0.952948
trainer/Policy Action Min                          -0.929176
exploration/num steps total                     12000
exploration/num paths total                        12
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.136363
exploration/Rewards Std                             0.420963
exploration/Rewards Max                             1.67808
exploration/Rewards Min                            -1.45785
exploration/Returns Mean                         -136.363
exploration/Returns Std                             0
exploration/Returns Max                          -136.363
exploration/Returns Min                          -136.363
exploration/Actions Mean                           -0.113114
exploration/Actions Std                             0.592851
exploration/Actions Max                             1
exploration/Actions Min                            -1
exploration/Num Paths                               1
exploration/Average Returns                      -136.363
exploration/env_infos/final/reward_run Mean         0.729992
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.729992
exploration/env_infos/final/reward_run Min          0.729992
exploration/env_infos/initial/reward_run Mean       0.959313
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.959313
exploration/env_infos/initial/reward_run Min        0.959313
exploration/env_infos/reward_run Mean               0.0821976
exploration/env_infos/reward_run Std                0.401445
exploration/env_infos/reward_run Max                1.83467
exploration/env_infos/reward_run Min               -1.23408
exploration/env_infos/final/reward_ctrl Mean       -0.19417
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.19417
exploration/env_infos/final/reward_ctrl Min        -0.19417
exploration/env_infos/initial/reward_ctrl Mean     -0.316561
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.316561
exploration/env_infos/initial/reward_ctrl Min      -0.316561
exploration/env_infos/reward_ctrl Mean             -0.21856
exploration/env_infos/reward_ctrl Std               0.0902318
exploration/env_infos/reward_ctrl Max              -0.0197691
exploration/env_infos/reward_ctrl Min              -0.530146
evaluation/num steps total                       2000
evaluation/num paths total                          2
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.035318
evaluation/Rewards Std                              0.495676
evaluation/Rewards Max                              0.949405
evaluation/Rewards Min                             -1.31278
evaluation/Returns Mean                           -35.318
evaluation/Returns Std                              0
evaluation/Returns Max                            -35.318
evaluation/Returns Min                            -35.318
evaluation/Actions Mean                             0.00390037
evaluation/Actions Std                              0.362455
evaluation/Actions Max                              0.955191
evaluation/Actions Min                             -0.946994
evaluation/Num Paths                                1
evaluation/Average Returns                        -35.318
evaluation/env_infos/final/reward_run Mean         -0.44962
evaluation/env_infos/final/reward_run Std           0
evaluation/env_infos/final/reward_run Max          -0.44962
evaluation/env_infos/final/reward_run Min          -0.44962
evaluation/env_infos/initial/reward_run Mean        0.536909
evaluation/env_infos/initial/reward_run Std         0
evaluation/env_infos/initial/reward_run Max         0.536909
evaluation/env_infos/initial/reward_run Min         0.536909
evaluation/env_infos/reward_run Mean                0.0435152
evaluation/env_infos/reward_run Std                 0.460481
evaluation/env_infos/reward_run Max                 1.02679
evaluation/env_infos/reward_run Min                -1.25433
evaluation/env_infos/final/reward_ctrl Mean        -0.329803
evaluation/env_infos/final/reward_ctrl Std          0
evaluation/env_infos/final/reward_ctrl Max         -0.329803
evaluation/env_infos/final/reward_ctrl Min         -0.329803
evaluation/env_infos/initial/reward_ctrl Mean      -0.118575
evaluation/env_infos/initial/reward_ctrl Std        0
evaluation/env_infos/initial/reward_ctrl Max       -0.118575
evaluation/env_infos/initial/reward_ctrl Min       -0.118575
evaluation/env_infos/reward_ctrl Mean              -0.0788332
evaluation/env_infos/reward_ctrl Std                0.0794368
evaluation/env_infos/reward_ctrl Max               -0.00956297
evaluation/env_infos/reward_ctrl Min               -0.374825
time/data storing (s)                               0.00458123
time/evaluation sampling (s)                        0.193395
time/exploration sampling (s)                       0.224364
time/logging (s)                                    0.00754087
time/saving (s)                                     0.00305096
time/training (s)                                   9.93357
time/epoch (s)                                     10.3665
time/total (s)                                     23.1434
Epoch                                               1
----------------------------------------------  ---------------
