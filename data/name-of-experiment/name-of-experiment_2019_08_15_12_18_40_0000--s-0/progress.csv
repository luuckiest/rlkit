evaluation/Returns Min,trainer/Policy Action Min,evaluation/Actions Max,time/exploration sampling (s),exploration/env_infos/initial/reward_run Mean,exploration/env_infos/initial/reward_run Std,evaluation/env_infos/initial/reward_ctrl Std,exploration/Returns Std,trainer/Bellman Errors Min,evaluation/Rewards Min,evaluation/env_infos/final/reward_run Min,time/data storing (s),exploration/Returns Mean,time/logging (s),evaluation/env_infos/initial/reward_ctrl Min,evaluation/Returns Mean,evaluation/env_infos/reward_ctrl Min,evaluation/num paths total,exploration/env_infos/reward_run Std,exploration/env_infos/final/reward_run Max,time/training (s),exploration/Actions Max,Epoch,exploration/env_infos/reward_run Min,trainer/Policy Action Mean,trainer/Bellman Errors Std,evaluation/env_infos/reward_run Min,exploration/Actions Min,trainer/Raw Policy Loss,exploration/Num Paths,trainer/Policy Action Std,evaluation/path length Min,evaluation/path length Max,evaluation/Actions Min,exploration/env_infos/initial/reward_ctrl Min,exploration/env_infos/final/reward_run Mean,exploration/path length Std,exploration/env_infos/reward_run Mean,exploration/env_infos/final/reward_run Min,trainer/QF Loss,exploration/path length Max,evaluation/Average Returns,evaluation/env_infos/initial/reward_ctrl Max,time/evaluation sampling (s),exploration/Rewards Min,exploration/env_infos/final/reward_run Std,exploration/env_infos/reward_ctrl Min,exploration/env_infos/final/reward_ctrl Mean,exploration/Rewards Std,exploration/env_infos/initial/reward_ctrl Std,trainer/Q Predictions Std,trainer/Q Predictions Max,evaluation/Rewards Max,evaluation/env_infos/initial/reward_run Max,evaluation/env_infos/final/reward_ctrl Mean,exploration/env_infos/final/reward_ctrl Min,time/saving (s),exploration/env_infos/reward_ctrl Mean,exploration/Actions Std,evaluation/env_infos/reward_run Mean,evaluation/Actions Std,exploration/num steps total,exploration/env_infos/initial/reward_ctrl Max,exploration/env_infos/final/reward_ctrl Std,evaluation/env_infos/reward_ctrl Max,evaluation/path length Mean,exploration/env_infos/reward_ctrl Max,evaluation/env_infos/reward_ctrl Mean,exploration/env_infos/reward_ctrl Std,evaluation/env_infos/final/reward_ctrl Max,evaluation/env_infos/final/reward_ctrl Min,exploration/Returns Max,evaluation/env_infos/initial/reward_run Std,evaluation/Rewards Mean,evaluation/env_infos/initial/reward_ctrl Mean,evaluation/env_infos/reward_run Std,exploration/env_infos/final/reward_ctrl Max,exploration/Rewards Max,evaluation/env_infos/final/reward_run Std,trainer/Policy Action Max,evaluation/Rewards Std,exploration/Average Returns,trainer/Q Targets Std,evaluation/env_infos/initial/reward_run Mean,exploration/Actions Mean,trainer/Q Predictions Min,evaluation/path length Std,exploration/env_infos/initial/reward_ctrl Mean,trainer/Policy Loss,evaluation/Returns Max,exploration/Returns Min,evaluation/env_infos/reward_run Max,trainer/Q Targets Max,evaluation/env_infos/reward_ctrl Std,time/total (s),evaluation/Actions Mean,evaluation/env_infos/final/reward_ctrl Std,time/epoch (s),evaluation/Num Paths,evaluation/num steps total,exploration/num paths total,exploration/Rewards Mean,evaluation/env_infos/final/reward_run Mean,trainer/Q Targets Mean,trainer/Preactivation Policy Loss,evaluation/Returns Std,exploration/path length Min,exploration/env_infos/initial/reward_run Min,exploration/env_infos/initial/reward_run Max,exploration/path length Mean,trainer/Bellman Errors Mean,evaluation/env_infos/final/reward_run Max,trainer/Bellman Errors Max,trainer/Q Predictions Mean,trainer/Q Targets Min,exploration/env_infos/reward_run Max,replay_buffer/size,evaluation/env_infos/initial/reward_run Min
0.3968928109414584,-0.0060805795,0.006566052,0.2249793240000031,-0.12201543453371794,0.0,0.0,0.0,4.38855e-06,-0.0918811437085498,-1.3877787807814457e-16,0.004519994000020233,-158.74211798444722,0.006006501000001663,-4.98742883792147e-06,0.3968928109414584,-7.265277236001567e-06,1,0.31480746491638845,0.08584300300894077,9.308614282000008,1.0,0,-1.6826808381379132,0.0023748765,0.27446294,-0.09187644712520357,-1.0,-0.000846647,1,0.0034694374,1000,1000,-0.0020203032,-0.05420312557440145,0.08584300300894077,0.0,0.015503269593687034,0.08584300300894077,0.17646582,1000,0.3968928109414584,-4.98742883792147e-06,0.19199265199995352,-1.8598115585363841,0.0,-0.4924423810161077,-0.1622263255642946,0.32793446554096956,0.0,0.0018823915,0.0057158917,0.16912839965164198,0.09653839970877154,-5.197104474063963e-06,-0.1622263255642946,0.01036026200000606,-0.1742453875781343,0.5386668769970223,0.00040208813453649505,0.0024430074,11000,-0.05420312557440145,0.0,-3.7813955714227635e-06,1000.0,-0.01152436282045396,-5.1953235950350055e-06,0.07519565287210135,-5.197104474063963e-06,-5.197104474063963e-06,-158.74211798444722,0.0,0.0003968928109414601,-4.98742883792147e-06,0.009416315315866333,-0.1622263255642946,0.9839244836218085,0.0,0.011719117,0.009416269542917521,-158.74211798444722,0.3793058,0.09653839970877154,-0.01571543582485995,-0.0032841028,0.0,-0.05420312557440145,-0.000846647,0.3968928109414584,-158.74211798444722,0.1691343245152227,1.0077097,1.020150058183959e-07,12.773434656999996,0.0016403141,0.0,9.746473014999992,1,1000,11,-0.15874211798444723,-1.3877787807814457e-16,-0.18006508,0.0,0.0,1000,-0.12201543453371794,-0.12201543453371794,1000.0,0.17646582,-1.3877787807814457e-16,2.1225548,0.0009293142,-1.4577769,1.1641710109918724,11000,0.09653839970877154
-35.318016632314425,-0.929176,0.95519084,0.22436377799999718,0.9593127216458086,0.0,0.0,0.0,6.156623e-05,-1.3127761349219609,-0.44962047821811346,0.0045812280000063765,-136.36267008222777,0.007540867999978218,-0.11857450008392334,-35.318016632314425,-0.37482519149780275,2,0.4014450387517792,0.7299918402602046,9.933574988000032,1.0,1,-1.2340764761001388,-0.10383267,0.5912861,-1.254328739530921,-1.0,-0.28093007,1,0.3780543,1000,1000,-0.94699365,-0.3165611063860904,0.7299918402602046,0.0,0.08219761254273589,0.7299918402602046,0.23506308,1000,-35.318016632314425,-0.11857450008392334,0.19339466200000288,-1.4578524468162701,0.0,-0.5301458051182971,-0.19416987726428547,0.4209627684912812,0.0,2.167659,6.8052406,0.9494053254884719,0.5369090034053995,-0.3298026561737061,-0.19416987726428547,0.0030509649999999056,-0.21856028262496352,0.5928509960945616,0.043515199763659695,0.3624548,12000,-0.3165611063860904,0.0,-0.009562968462705613,1000.0,-0.01976914257535145,-0.07883321639597418,0.09023175207797786,-0.3298026561737061,-0.3298026561737061,-136.36267008222777,0.0,-0.035318016632314465,-0.11857450008392334,0.46048149931381366,-0.19416987726428547,1.6780814841834906,0.0,0.95294803,0.49567649730652164,-136.36267008222777,2.2607615,0.5369090034053995,-0.11311425258542185,-7.4084215,0.0,-0.3165611063860904,-0.28093007,-35.318016632314425,-136.36267008222777,1.0267861534822198,6.286345,0.07943677310135507,23.143366857000046,0.003900373,0.0,10.366506489000017,1,2000,12,-0.13636267008222763,-0.44962047821811346,0.23832673,0.0,0.0,1000,0.9593127216458086,0.9593127216458086,1000.0,0.23506308,-0.44962047821811346,4.5642967,0.015798133,-7.521507,1.8346716687712838,12000,0.5369090034053995
